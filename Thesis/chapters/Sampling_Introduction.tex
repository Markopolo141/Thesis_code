\chapter{Stratified Sampling}\label{chap:stratified_sampling_chapter}

In the previous chapter we introduced the GNK value and recognised that it could be approximated by a stratified sampling process (section \ref{sec:sampling_techniques}).
Accordingly, in this chapter we investigate the different possible rules for choosing samples from strata in the context of stratified sampling.

We review the orthodox solution to the problem which is called Neyman allocation, and show how it is equivalent to minimising a concentration inequality called Chebyshev's ineqality.
We then turn our attention to two different approaches to developing novel concentration inequalities for stratified sampling, whose minimisation yields new stratified sampling methods.
These new sampling methodologies are then tested for their performance in the context of synthetic sample data sets, and in the context of sampling the Shapley Value of cooperative games.

The structure of this Chapter is as follows:
\begin{enumerate}
\item	In section \ref{sec:sampling_background}, we give a review of relevant background information and techniques, particularly addressing the recent innovation of Empirical Bernstein Bounds (EBBs).
\item	In section \ref{section:old_statistics}, of this chapter we show how EBBs can be bound together to create concentration inequalities appropriate for Stratified Sampling and give an algorithm for choosing samples to minimise these bounds.
\item	In section \ref{section:new_EBB} of this chapter we derive a unique and stronger EBB, for the purposes of evaluation in the context of stratified sampling.
\item	In section \ref{section:SEBB}, we provide multi-part derivations of complicated concentration inequalities specifically tailored for the purposes of stratified sampling, and give an algorithm for choosing samples to minimise these inequalities.
\item	In section \ref{section:statistics_results}, we give the performance results of various methods of stratified sampling in the context of synthetic data sets, and in the application of approximating the Shapley Value.
\item	In section \ref{sec:discussion} we give discussion to the derivations and results, and talk briefly about future work about a multidimensional stratified sampling EBB.
\end{enumerate}


The work and development from the second and third section most directly reflects published material: ``An Engineered Empirical Bernstein Bound'',\\ European Conference on Machine Learning (ECML-PKDD) 2019\\(accessible: \href{http://ecmlpkdd2019.org/downloads/paper/435.pdf}{ecmlpkdd2019.org/downloads/paper/435.pdf})\\
The work and development from the fourth and fifth section most directly reflect unpublished material: ``Stratified Finite Empirical Bernstein Sampling'', Preprints 2019\\
(accessible: \href{preprints.org/manuscript/201901.0202/v2}{preprints.org/manuscript/201901.0202/v2})\\



%In the same way as the average of a statistic over the population of a country is expressible as the weighted average over the averages of its regions, so to the Shapley Value and GNK value are expressible as a weighted average over the averages of coalitions by different sizes.
%Hence component of our investigation turns fundamentally into a question about stratified sampling.

%Particularly, when we have a population decomposed into strata of known sizes which we can selectively sample from, how do we choose samples from the strata to get the most accuracy in the final population estimate?

%This question is directly relevant to sampling the GNK and Shapley value, since they are both immediately expressible as a weighted average of strata averages, where the strata and their sizes are immediately given. And in the context of sampling the Shapley Value, the question of which strata should sampled is equivalent to a question about what sized coalitions should be sampled in consideration of each players marginal contributions to them.


\section{Introduction and background}\label{sec:sampling_background}

%In the previous chapter, we defined the GNK value and saw that there is a significant computational difficulties in solving the GNK value at scale.
%Particularly that the GNK value and its Shapley Value formulation are definable as being an average over a combinatorial number of averages (via equations \ref{da_value_eq} or \ref{eq:shapley_value2} by conversion identified by equation \ref{convert1}).

%It was realised that these averages might be approximated by sample averages, and hence that one potential avenue of approximating the GNK value at scale was by sampling.
%And the process of replacing analytic averages with sample averages leads to the question of how that sampling process could be done most effectively.

Stratified sampling is a well known example of a process of selecting samples to most accurately estimate an average over weighted sample averages.
Particularly stratified random sampling is a process of estimating the average over a population by breaking a population into mutually-exclusive subgroups and sampling them randomly.
Such stratified sampling has been identified to lead to improved reliability in estimation over simple random sampling of the population by \cite{1938.10503378,10.2307/23339498} particularly when:
\begin{itemize}
\item The population is divisible into strata, in which there is less variance in each stratum than across them all 
\item When the size of the strata are known or reasonably estimated
\item When sampling selectively from each strata is possible 
\end{itemize}

The concept of stratified random sampling is easily illustrated, for instance, to poll the population of a country's support for a particular government policy, it is possible to sample the different demographic regions within the country.
For instance, if regions $A$,$B$ and $C$ contain 10\%, 40\% and 50\% of a population, and sampling of these regions reliably show support levels of 2\%, 70\% and 30\%, respectively, then it is possible to discern that 43.2\% of the total population supports the policy.

Our primary investigative question of this chapter is this: when we have a population decomposed into strata of known sizes which we can selectively sample from, how do we choose samples from the strata to get the most accuracy in the final population estimate?

For instance, if we sample primarily from a single stratum, we would likely have a very good estimate of the average for that one stratum, but no accuracy in estimate for the others, leading to a weak estimate of the population average.
Conversely, if we choose to take the same number of samples from all the strata, some of the strata might be far smaller or have far less variance than others, resulting in those strata being oversampled.


%The core question we face is, how do we sample optimally between the strata in stratified sampling?
This question has been considered before, and the most direct historical answer to the question is called \textit{Neyman allocation} or \textit{sampling} \citep{1938.10503378}.
The principle of Neyman sampling is that it seeks the minimise the weighted variance of the population estimate assuming knowledge of the variances of the strata, and it can most directly be interpreted as a process of minimising Chebyschev's inequality.

\subsection{Neyman allocation}\label{sec:neyman_sampling}

Neyman allocation identifies that in order to minimise the variance in the final estimate of the population, that sampling should be directly in proportion to the stratum variances multiplied by their sizes:

\begin{theorem}[Neyman allocation]\label{thm:neyman_selection}
For $m$ strata, of sizes $N_i$, with variance $\sigma_i^2$. For a sample budget $n$, and there is a choice how much to sample from each strata $n_i$, in which sampling is done with replacement (ie. all samples are independent and identically distributed)
Then the selection of $n_i$ which minimises the variance of our population estimate $\mu$ is:
$$n_i = \frac{nN_i\sigma_i}{\sum_jN_j\sigma_j}$$
\end{theorem}
\begin{proof}
For any independant random variables $X$ and $Y$ (and for any $a,b\in \mathbb{R}$), $\text{Var}(aX+bY) = a^2\text{Var}(X)+b^2\text{Var}(Y)$.\\
So, if $X_{i,j}$ are the random variable of the $j$th sample from the $i$th stratum, then:
\begin{equation}\label{eq:variance_decomposition_for_strata_mean} \hat{\mu} = \sum_{i=1}^m\frac{N_i}{\sum_kN_k}\frac{1}{n_i}\sum_{j=1}^{n_i}X_{i,j} ~~~~~~~\text{and hence}~~~~~~~\text{Var}(\hat{\mu}) = \frac{1}{\left(\sum_kN_k\right)^2} \sum_{i=1}^m\frac{N_i^2}{n_i}\sigma_i^2 \end{equation}
To minimise the variance of our estimate $\text{Var}(\mu)$ by selecting $n_i$ subject to the constraint that $\sum_{i=1}^m n_i = n$, we form the Lagrangian (with Lagrange multiplier $\lambda$):
$$L = \text{Var}(\hat{\mu}) + \lambda\left(\sum_{i=1}^mn_i-n\right) = \sum_{i=1}^m\left(\frac{N_i^2\sigma_i^2}{n_i\left(\sum_kN_k\right)^2} + \lambda \left(n_i-\frac{n}{m}\right)\right)$$
Hence for any $i$ solving for $\frac{\partial L}{\partial n_i}=0$ leads to:
$n_i = \frac{N_i\sigma_i}{\lambda\sum_kN_k}$ and using $\sum_{i=1}^m n_i = n$, to eliminate $\lambda$ gives the result
\end{proof}

This allocation rule begs the question of why we would specifically want to minimise the variance of our population mean estimate.
And the primary reason is that that the variance of an estimate bounds the probability of error in the estimate, and this can be seen most directly by Chebyshev's inequality.

\begin{theorem}[Chebyshev's inequality]\label{thm:chebyshevs}
for any random variable --- in this case $\hat{\mu}$ is a random variable --- with variance $\text{Var}(\hat{\mu})$ then the error of $\hat{\mu}$ from its mean is probability bounded:
$$ \p\left(|\hat{\mu}-\mu|\ge k\sqrt{\text{Var}(\hat{\mu})}\right)\le\frac{1}{k^2} $$
\end{theorem}

Thus if $\mu$ is the true population mean, then the error in our stratified estimate of the population mean $\hat{\mu}$ is probability bounded by the variance of it.
So, for instance, Chebyshev's inequality guarantees that there will always be less than a 25\% chance that the error in our estimate of the population mean will be more than twice the square root of its variance.
In this context, Chebyshev's is an example of a \textit{concentration inequality}, as it can be used to provides probability bounds on the concentration of the estimate around its mean value.

An additional and perhaps more intuitive angle by which Neyman allocation may be seen to be appropriate, is that if sufficiently large numbers of samples have been taken then the sample means of the strata will tend to be Gaussian distributed by the Central Limit Theorem.
In this context the strata means have a distribution that is entirely characterised by their mean and variance, and hence so too therefore is the population mean.
In this context the variance of the sampled population mean is the only parameter controlling the error, and minimising it directly translates into improved accuracy.

One primary limitation of using Neyman allocation, is that is presupposes knowledge of the variances of the strata, which usually aren't available beforehand or in practice.
One relatively easy way of going around this problem is to go through a process to estimate the variances of the strata, perhaps as a prior step, and then using this knowledge with Neyman allocation to choose further samples.
%And indeed some proceses do this, particularly in Section \ref{sec:shapley} we will consider a method of sampling the Shapley Value using exactly this idea.
And while this idea certainly works, it leaves open the question about how much sampling should be done to estimate the strata variances against how much sampling should be left to sample by those estimated variances.

Neyman allocation is an orthodox sampling rule which directly extends from minimising a specific concentration inequality that unfortunately depends on known variances.
It is therefore suitable to ask if there are alternative concentration inequalities which do not depend on known variances, which can be minimised to form novel sampling rules for stratified sampling.




\subsection{Concentration inequalities and Chernoff bounds}

In order to consider what alternative concentration inequalities exist, which apply in stratified sampling and do not depend on known variances we need to turn to the space of concentration inequalities generally. 

Neyman allocation implicitly minimises Chebyshev's inequality, but Chebyshev's inequality is one example of many \textit{concentration inequalities}.
Concentration inequalities are applied in a range of data science contexts for a variety of prediction, machine learning and hypothesis testing tasks, including:
change detection \citep{KiferShaiGehrke2004,8000571} 
and classification \citep{Zia-UrRehman2012} in data streams;
outlier analysis in large databases \citep{Aggarwal2015};
online optimisation \citep{FlaxmanKalaiMcMahan2005,AgarwalDekelXiao2010}; and, of most relevance to this paper, 
online prediction and learning problems \citep{%Cesa-BianchiLugosi2006,%Maron1997,
Mnih:2008:EBS:1390156.1390241,DBLP:conf/aaai/ThomasTG15,Maurer50empiricalbernstein},
and in settings with bandit feedback \citep{AuerCesa-BianchiEtal_SIAM2003,AudibertBubeck_COLT2009,Tran-ThanhChapmanRJ_AAAI2009}.

There are many famous concentration inequalities such as Chebyshev's inequality~\citep{Chebyshev1}, Bernstein's inequalities~\citep{Burnstein1}, 
Hoeffding's inequalities~\citep{hoeffding1} and Bennett's inequalities~\citep{10.2307/2282438}.
New analysis has yielded a wider range of concentration inequalities and methods of generating them ~\citep{MR3363542,Boucheron2004}. 
In particular, various innovations concern the concentration of more-general functions of random variables, such as 
the Efron-Stein~\citep{efron1981} and 
entropy methods~\citep{Boucheron_concentrationinequalities}.%, and Talagrand's concentration inequality~\cite{Talagrand1995}.

Recently, concentration inequalities have been developed which do not rely on variance information, but incorporate uncertainty about variance information via the sample variance, these bounds are sometimes called \textit{Empirical Bernstein Bounds} (EBB).
These concentration inequalities describe the likely difference of a sample mean from the population mean in terms of the \textit{sample} variance, some of the first EBBs given in literature are:

\begin{theorem}[\cite{Maurer50empiricalbernstein}]\label{MandPsEBB}
Let $X$ be a real-valued random variable that is bounded $a\le X\le b$, with $D=b-a$.  Then for $x_1,x_2,\dots,x_n$ independent samples of $X$ the mean $\hat{\mu}=\frac{1}{n}\sum_{i=1}^nx_i$ and sample variance $\hat{\sigma}^2=\frac{1}{n}\sum_{i=1}^n(x_i-\hat{\mu})^2 $ are probability bounded by $t$ for any $t>0$:
\begin{equation}\label{maurersbound} 
    \p\left(\mu-\hat{\mu}\ge\sqrt{\frac{2\hat{\sigma}^2\log(2/t)}{n}}+\frac{7D\log(2/t)}{3(n-1)}\right)\le t
\end{equation}
\end{theorem}

\begin{theorem}[\cite{10.1007/978-3-540-75225-7_15}]\label{AudibertsEBB}
In exactly the same context as in Theorem \ref{MandPsEBB}
\begin{equation}
    \p\left(\mu-\hat{\mu}\ge\sqrt{\frac{2\hat{\sigma}^2\log(3/t)}{2n}} + \frac{3D\log(3/t)}{2n}\right) \le t.
    \end{equation}
\end{theorem}

In later section \ref{section:unionising_ebbs} we will show how these types of inequalities can be modified and subsequently minimised to create new methods of sample selection in stratified random sampling, and these new methods will not depend on knowledge of the variances proper (unlike Neyman sampling) but only the sample variances of the strata.
Unfortunately these EBBs cannot directly be used to minimise the error in the aggregated population estimate, as the aggregated population estimate does not itself have a sample variance.

In section \ref{derivation} we show how it is possible to derive stronger EBBs using classical techniques, hence it is pertinent to introduce the components used in the process of deriving EBBs.
One of the more famous Concentration Inequalities is \textit{Hoeffding's Inequality} which is one of a class of concentration inequalities called \textit{Chernoff bounds}.

\begin{lemma}[Chernoff Bound]\label{chernoff1}
If $\hat{\mu}$ is sample mean of $n$ independent and identically distributed samples of random variable $X$ then for any $s>0$ and $t$:
\[ \p(\hat{\mu}\ge t)\le\E\left[\exp(sX)\right]^n\exp(-snt) \]
\end{lemma}
\begin{proof}[Proof of Chernoff Bound - Lemma \ref{chernoff1}]
$$\p(\hat{\mu}\ge t) =  \p\left(\exp\left(s\sum_{i=1}^nx_i\right)\ge \exp(snt)\right)$$
$$\le \E\left[\exp\left(s\sum_{i=1}^nx_i\right)\right]\exp(-snt) \le \E\left[\exp\left(sX\right)\right]^n\exp(-snt)
$$
using Markov's inequality and the i.i.d of the samples, respectively.
\end{proof}

Here we have given the general form of Chernoff bounds for the sample mean of random variable, parameterised by a choice of $s>0$ and $t$.
Using this kind of lemma, many well-known examples of Chernoff bounds follow from the derivation of upper bounds for $\E\left[\exp(sX)\right]$, also known as the \textit{moment generating function}.

For any upper bound $\E[\exp(sX)]\le g(s)$ then $\p(\hat{\mu}>t)\le g^n(s)\exp(-snt)$ is an upper bound for the deviation of the sample mean, which can then be subsequently minimised with $s$, forming a concentration inequality.
And thus, a Chernoff concentration inequality can be deduced from a bound on the moment-generating-function.
Hoeffding's inequality \citep{hoeffding1} is an illustrative example of this process of deriving a Chernoff bound:

\begin{theorem}[Hoeffding's inequality for mean zero]\label{hoeffdings_inequality}
Let $X$ be a real-valued random variable that is bounded $a\le X\le b$, with a mean $\mu$ of zero.  Then for $D=b-a$ and any $t>0$, the mean $\hat{\mu}$ of $n$ independent samples of $X$ is probability bounded by:
\begin{equation}\label{eq_no2}\p(\hat{\mu}\ge t)\le \exp\left(\frac{-2nt^2}{D^2}\right)
\end{equation}
\end{theorem}
\begin{proof}
If $X$ has a probability density function $f(x)$, then we can linearise $\exp(sx)$ as:
\begin{equation}\label{Hoeffdings_line_fitting}\E[\exp(sX)] = \int_a^bf(x)\exp(sx)dx \le \int_a^bf(x)\left(\frac{x-a}{b-a}\exp(sb) + \frac{b-x}{b-a}\exp(sa)\right)dx\end{equation}
Using the fact that the mean $\mu = \int_a^bf(x)xdx = 0$ thus:
\begin{equation}\E[\exp(sX)] \le \frac{1}{sb-sa}\left(sb\exp(sa) - sa\exp(sb) \right)\end{equation}
Given the fact that for any $\kappa>0,\gamma<0$:
\begin{equation}\label{Hoeffdings_lemma} \frac{\kappa\exp(\gamma)-\gamma\exp(\kappa)}{\kappa-\gamma}\le \exp\left(\frac{1}{8}(\kappa-\gamma)^2\right) \end{equation}
thus:
\begin{equation}\label{hoeffdings_lemma_eq}\E[\exp(sX)] \le \exp\left(\frac{1}{8}s^2(b-a)^2 \right)\end{equation}
Applying our Chernoff bound lemma \ref{chernoff1} we get:
$$ \p(\hat{\mu}\ge t) \le \exp\left(\frac{1}{8}s^2(b-a)^2 n-snt\right) $$
And minimising with respect to $s$ yields the required result.
\end{proof}

The most limiting feature of the derivation is the requirement that the mean is zero however this is immaterial and is used to simplify the derivation, as any statistic can be shifted such that its expectation value becomes zero. hence:

\begin{theorem}[Hoeffding's inequality]\label{Hoeffdings_inequality_proper}
Let $X$ be a real-valued random variable that is bounded $a\le X\le b$.  Then for $D=b-a$ and any $t>0$, the mean $\hat{\mu}$ of $n$ independent samples of $X$ is probability bounded by:
\begin{equation}\label{eq_no2}\p(\hat{\mu}-\mu\ge t)\le \exp\left(\frac{-2nt^2}{D^2}\right)
\end{equation}
Alternatively by rearranging:
\begin{equation}\label{eq_no2}\p\left(\hat{\mu}-\mu\ge \sqrt{\frac{D^2\log(1/t)}{2n}}\right)\le t
\end{equation}
\end{theorem}

%A limiting feature of Hoeffding's inequality is that it assumes that the data are bounded within a range $a\le X\le b$, which limits the application of Hoeffding's inequality.
%Similarly Chebyschev's inequality only sensibly holds in contexts where the data's distribution has a defined variance (consider that various heavy-tailed distributions may not have a finite variance, for instance).
%What is quite notable is that it is difficult to come to very meaningful conclusions about data processes without assuming atleast -something- about the distribution of data in question.
%And further that these inequalities can be seen as being largely a consequence of chosen assumptions.
%These assumptions may be based on prior information, expert opinion, or determined from the characteristics of the system under observation.

It is also worth noting that Hoeffding's inequality is a rather friendly result that states that the concentration of sample mean is probability bounded by a Gaussian function, which might be seen as an intuitive corollary of the Central Limit Theorem. 
We give its derivation to illustrate the technique of deriving a Chernoff bound, additionally we will utilise Equation \ref{hoeffdings_lemma_eq} in further derivations - also called Hoeffding's Lemma\footnote{It is also worth noting that this lemma is actually unnecessary in the derivation of Hoeffding's inequality, and failing to incorporate it ultimately results in a stronger but uglier bound - see Appendix \ref{Appendix:more_concentration}}:

\begin{lemma}[Hoeffding's Lemma]\label{Hoeffdings_lemma_lemma}
Let $X$ be a real-valued random variable that is bounded $a\le X\le b$, with a mean $\mu$ of zero, then for $D=b-a$ and any $s>0$:
$$\E[\exp(sX)] \le \exp\left(\frac{1}{8}s^2D^2 \right)$$
\end{lemma}

This process of deriving and minimising a bound for the moment-generating-function will be used repeatedly to create novel concentration inequalities in sections \ref{section:old_statistics} and \ref{section:SEBB}.



\subsubsection{A further note Chernoff bounds, sampling with or without replacement}

In many cases, the derivation of concentration inequalities assume that the values that the samples take are independently from each other. 
This most naturally corresponds to the schema of sampling \textit{with} replacement, rather than sampling \textit{without} replacement.
However Chernoff probability bounds that assume independence of the samples are also suitable to the case of sampling without replacement, this is due to a result shown by \cite{hoeffding1}:

\begin{lemma}[Hoeffding's reduction]\label{hoeffdings_reduction}
let $X=(x_1,\dots,x_n)$ be a finite population of $n$ real points, let $X_1,\dots,X_n$ denote random samples without replacement from $X$ and $Y_1,\dots,Y_n$ denote random samples with replacement from $X$. 
If $f:\mathbb{R}\rightarrow\mathbb{R}$ is continuous and convex, then:
$$\textstyle \E\left[f\left(\sum_{i=1}^nX_i\right)\right]\le\E\left[f\left(\sum_{i=1}^nY_i\right)\right] $$
\end{lemma}

Using the continuous and convex function $f(x)=\exp(sx)$ it can be seen via the construction of Chernoff bounds (Theorem \ref{chernoff1}) that this result implies that all Chernoff bounds developed for sampling with replacement also hold for sampling without replacement.

%Concentration inequalities that specifically describe sampling without replacement can give additional tightening over those that assume sampling with replacement. This refinement was first demonstrated in \cite{serfling1974} with a martingale argument.
%More recently, this result was improved with a reverse martingale argument and used to extend M\&P's EBB to the case of sampling without replacement \cite{bardenet2015}.
%These techniques are extended in section \ref{sec:without_replacement} we specifically give a tightening result specific to sampling without replacement, but it is sufficient to note that all bounds developed here for sampling with replacement are suitable for sampling without replacement by this Lemma.


\subsection{Other general probability lemmas}

There are further Lemmas that are necessary for the further derivations for this Chapter.
The first lemma is an often-used and rather weak result used to fuse simple statements of probability:
\begin{lemma}[Probability Union]\label{prob_union}
For any random variables $a,b$ and $c$:
\[\p(a>c) \; \le \; \p(a>b) + \p(b>c)\]
\end{lemma}
\begin{proof}[Proof of Probability Union - Lemma \ref{prob_union}]
For events $A$ and $B$\\$\p(A\cup B) \le \p(A)+\p(B)$\\
hence for events $a>b$ and $b>c$:\\ $\p((a>b) \cup (b>c)) \le \p(a>b) + \p(b>c)$\\
If $a>c$, then $(a>b) \cup (b>c)$ is true irrespective of $b$, so:\\
$\p(a>c) \le \p((a>b) \cup (b>c))$
\end{proof}
This relationship is a well known and useful tool for settings where the probability relationship between $a$ and $c$ is unknown but the relationship between $a$ and some $b$, and also between that $b$ and $c$ is known.
Although this relationship is quite useful, it is known to be a very weak relationship, as it holds with equality (ie. $\p(a>c)=\p(a>b)+\p(b>c)$) only if $\p((a>b)\cap(b>c))=0$.

Note also, that the same proof method for probability union also works straightforwardly for various substitutions of the $\ge$ symbol for the $>$ in the inner inequalities, with the exception that: $\p(a\ge c) \; \le \; \p(a>b) + \p(b>c)$ may be false.\footnote{ie. In the context of lemma \ref{prob_union} the following inequalities can be proven by much the same logic $\p(a>c)\le\p(a\ge c)\le\p(a\ge b)+\p(b>c)$ and $\p(a>c)\le\p(a\ge c)\le\p(a\ge b)+\p(b\ge c)$, however care must be taken because $\p(a\ge c)\le\p(a>b)+\p(b>c)$ may not be true}

A second Lemma is a straightforward result of algebra that relates the sample squares about the mean and the mean squared, to the sample variance.
\begin{lemma}[Variance Decomposition]\label{variance1}
For $n$ samples $x_i$, sample mean $\hat{\mu} = \frac{1}{n}\sum_ix_i$, sample variance $\hat{\sigma}^2=\frac{1}{n-1}\sum_i(x_i-\hat{\mu})^2$, and average of sample squares $\hat{\sigma}_0^2 = \frac{1}{n}\sum_ix_i^2$, the following relationship holds:
% \begin{align*}
% \quad\hat{\sigma}^2 
% &=\frac{1}{n-1}\sum_i\left(x_i-\frac{1}{n}\sum_jx_j \right)^2 
%  =\frac{1}{n-1}\left(\sum_ix_i^2-\frac{1}{n}\sum_{i,j}x_ix_j \right) % \\
% &= \frac{n}{n-1}\left(\hat{\sigma}_0^2-\hat{\mu}^2\right)
% \end{align*}
% which rearranging gives:
\[
\hat{\sigma}_0^2=\hat{\mu}^2+\frac{n-1}{n}\hat{\sigma}^2
\]
\end{lemma}
\begin{proof}[Proof of Variance Decomposition - Lemma \ref{variance1}]
By expanding $\hat{\sigma}^2$ into parts:\\
\(\quad\hat{\sigma}^2=\frac{1}{n-1}\sum_i\left(x_i-\frac{1}{n}\sum_jx_j \right)^2 
=\frac{1}{n-1}\left(\sum_ix_i^2-\frac{1}{n}\sum_{i,j}x_ix_j \right) = \frac{n}{n-1}\left(\hat{\sigma}_0^2-\hat{\mu}^2\right)\)
\end{proof}

%And we will eventually use this result to create bounds for the sample variance from bounds on the sample squares about the mean and the mean squared.








\section{Some associated concentration inequalities}\label{Appendix:more_concentration}

\begin{theorem}\label{hoeffdings_inequality22}
Let $X$ be a real-valued random variable that is bounded $a\le X\le b$, with a mean $\mu$ of zero.  Then for $t>0$, the mean $\hat{\mu}$ of $n$ independent samples of $X$ is probability bounded by:
\begin{equation}\p(\hat{\mu}-\mu\ge t)\le \left( \frac{b}{b-a}\left(\frac{b(a-t)}{a(b-t)}\right)^{\frac{a-t}{b-a}} -\frac{a}{b-a}\left(\frac{b(a-t)}{a(b-t)}\right)^{\frac{b-t}{b-a}}  \right)^n
\end{equation}
\end{theorem}
\begin{proof}
Similar to the proof in Theorem \ref{hoeffdings_inequality} we follow the same steps except do not apply Equation \ref{Hoeffdings_lemma}, leading to:
$$ \p(\hat{\mu}\ge t) \le \left(\frac{b\exp(sa) - a\exp(sb)}{\exp(st)(b-a)}\right)^n $$
And minimising with respect to $s$ 
%occurs at, which 
yields the required result.
%$$ s = \frac{1}{b-a}\log\left(\frac{b(a - t)}{a(b - t)}\right) $$
\end{proof}
This concentration inequality is more powerful but more ugly and difficult to manipulate, it is also more commonly stated for variable $X$ with non-zero mean and bounded $0<X<1$:

\begin{theorem}[Also called Hoeffding's inequality]\label{hoeffdings_inequality23}
Let $X$ be a real-valued random variable that is bounded $0\le X\le 1$, with mean $\mu$. Then for $t>0$, the mean $\hat{\mu}$ of $n$ independent samples of $X$ is probability bounded by:
\begin{equation}\p(\hat{\mu}-\mu\ge t)\le \left[\left(\frac{1-\mu}{1-t-\mu}\right)^{1-t-\mu}  \left(\frac{\mu}{t+\mu}\right)^{t+\mu}\right]^n
\end{equation}
\end{theorem}
Which follows directly from the substitution $a=-\mu$ and $b=1-\mu$.

Theorems \ref{hoeffdings_inequality22} and \ref{hoeffdings_inequality23} will not be used for further derivation, but to illustrate the point that 



%\section{An Efron-Stein inequality for the sample variance}\label{appendix:efron_chebyshev}

%It is to be noted that one way to derive a concentration inequality for the variance is to use Chebyshev's inequality for the sample variance itself.
%Consider that if $\hat{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\mu)^2$ where $\mu = \frac{1}{n}\sum_{i=1}^nx_i$ is the sample mean, then:

%$$\p\left(|\hat{\sigma}^2-\sigma^2|\ge k\right)\le \frac{\text{Var}(\hat{\sigma}^2)}{k^2}$$
%which gives us a pretty straightforward error on the sample variance if we can bound $\text{Var}(\hat{\sigma}^2)$

%Now since, $\hat{\sigma}^2$ is a general function of the samples $x_1,\dots,x_n$ we can apply Efron-Stein inequality to bound it.
%The Efron-Stein inequality bounds the variance of a function of random variables by the sum of variances about each of those parameter variables.

%\begin{theorem}[Efron-Stein inequality]
%If $f$ is a function of $n$ random variables $x_1,x_2,\dots,x_n$, let $x'_1,x'_2,\dots,x'_n$ be independent copies of the same variables, letting $Z=f(x_1,x_2,\dots,x_n)$ and $Z'_i=f(x_1,x_2,\dots,x_{i-1},x'_i,x_{i+1},\dots,x_n)$
%then:
%$$ \text{Var}(Z) \le \frac{1}{2}\sum_{i=1}^n\E[(Z-Z'_i)^2]$$
%\end{theorem}

%Applying the Efron-Stein inequality to the function $\hat{\sigma}^2$ gives:
%$$\text{Var}(\hat{\sigma}^2)\le \frac{5-n}{n(n-1)}\sigma^2 + \frac{1}{n}\mu_4 $$
%where $\mu_4$ is the forth central moment.

%Therefore there are two primary options, we can eliminate the first term, or we can half reduce the second.
%For the first option, the $\sigma^2$ coefficient is non-positive for $n\ge 5$, therefore for $n\ge 5$ that:
%$$\text{Var}(\hat{\sigma}^2)\le \frac{1}{n}\mu_4 $$ and given that if the variables $x_1,\dots,x_n$ are bounded $a\le X\le b$ with $D=b-a$ then
%$\mu_4\le \frac{D^4}{16}$ and hence:
%\begin{equation}\label{appendix_eq_1}\p\left(|\hat{\sigma}^2-\sigma^2|\ge k\right)\le \frac{D^4}{16nk^2}\end{equation}


%For the second option, since $\mu_4=\E[(X-\mu)^4]\le D^2\E[(X-\mu)^2]=D^2\sigma^2$ hence:
%$$\text{Var}(\hat{\sigma}^2)\le \frac{5-n}{n(n-1)}\sigma^2 + \frac{1}{n}\mu_4 \le \left(\frac{5-n}{n(n-1)} + \frac{D^2}{n}\right)\sigma^2$$
%hence:
%\begin{equation}\label{appendix_eq_2}\p\left(|\hat{\sigma}^2-\sigma^2|\ge k\right)\le \left(\frac{5-n}{n(n-1)} + \frac{D^2}{n}\right)\frac{\sigma^2}{k^2}
%\end{equation}

%Therefore combining these two expressions \ref{appendix_eq_1} and \ref{appendix_eq_2} becomes:

%\begin{equation}\label{appendix_eq_3}\p\left(|\hat{\sigma}^2-\sigma^2|\ge k\right)\le \min\left(\left(\frac{5-n}{n(n-1)} + \frac{D^2}{n}\right)\frac{\sigma^2}{k^2},\frac{D^4}{16nk^2}\right)
%\end{equation}
%Which is valid for $n\ge 5$.





